---
title: "Workflow Example using S3"
chapter: false
weight: 20
---

## Local Run using S3

The previous run was using the local file system for results. 

The script also alows to overwrite the `output` destination and thus, we can use an S3 bucket as a destination.

```
export BUCKET_NAME_RESULTS=nextflow-spot-batch-result-${RANDOM}-$(date +%s)
aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_RESULTS}
```

With the bucket created, we can add the parameter to the nextflow run:

```
nextflow run script7.nf --reads 'data/ggal/*_{1,2}.fq' --outdir=s3://${BUCKET_NAME_RESULTS}
```

The output will look like this.

```
$ nextflow run script7.nf --reads 'data/ggal/*_{1,2}.fq' --outdir=s3://${BUCKET_NAME_RESULTS}
N E X T F L O W  ~  version 20.01.0
Launching `script7.nf` [prickly_bose] - revision: ce58523d1d
R N A S E Q - N F   P I P E L I N E    
===================================
transcriptome: /home/ec2-user/environment/nextflow-tutorial/data/ggal/transcriptome.fa
reads        : data/ggal/*_{1,2}.fq
outdir       : s3://nextflow-spot-batch--10149-1587633328
executor >  local (8)
[66/0f4f9d] process > index          [100%] 1 of 1 ✔
[cd/7df0a0] process > quantification [100%] 3 of 3 ✔
[70/853999] process > fastqc         [100%] 3 of 3 ✔
[49/e9c227] process > multiqc        [100%] 1 of 1 ✔
Done! Open the following report in your browser --> s3://nextflow-spot-batch--10149-1587633328/multiqc_report.html
$ 
```
